{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and process baseline features for LIAR test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data LIAR test dataset\n",
    "src = 'data/liar_dataset/test.tsv'\n",
    "raw_data = pd.read_csv(src, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data.copy(deep=True)\n",
    "\n",
    "# Select only label and statment columns, remove the rest\n",
    "clean_data = clean_data[[1,2]]\n",
    "\n",
    "# rename columns to match FakeNews corpus\n",
    "clean_data = clean_data.rename(columns={1:'type'})\n",
    "clean_data = clean_data.rename(columns={2:'content'})\n",
    "\n",
    "# remove empty 'content' rows\n",
    "clean_data.dropna(subset=['content'], inplace=True)\n",
    "\n",
    "# remove rows without type labels\n",
    "drop_null_types = clean_data[ (clean_data['type'].isnull())].index\n",
    "clean_data.drop(drop_null_types, inplace=True)\n",
    "\n",
    "# remove types that are not 'true' or 'false'\n",
    "omitted_types = {\n",
    "    'half-true',\n",
    "    'mostly-true',\n",
    "    'barely-true',\n",
    "    'pants-fire'\n",
    "}\n",
    "drop_indexes = clean_data[ (clean_data['type'].isin(omitted_types))].index\n",
    "clean_data.drop(drop_indexes, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.process_methods as pm\n",
    "import swifter\n",
    "\n",
    "# cleanup text on 'content' column and add into new column 'content_clean'\n",
    "clean_data['content_clean'] = clean_data['content'].swifter.apply(pm.clean_text)\n",
    "\n",
    "# Apply remove_stopwords to 'content_clean' column and create 'content_stopword' column\n",
    "clean_data['content_stopword'] = clean_data['content_clean'].swifter.apply(pm.remove_stopwords)\n",
    "\n",
    "# stemming\n",
    "clean_data['content_stem'] = clean_data['content_stopword'].swifter.apply(pm.remove_word_variations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process baseline features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make grouped types ('fake' or 'reliable') into 'true' or 'false' values\n",
    "def bool_dummies(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    type_data = pd.get_dummies(df['type'], drop_first=True)\n",
    "    df = pd.concat([df, type_data], axis=1)\n",
    "    return df\n",
    "\n",
    "clean_data = bool_dummies(clean_data, 'type')\n",
    "clean_data = clean_data.rename(columns={'true':'reliable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to count tags, e.g. NUMs with <NUM> tag\n",
    "def count_tag(text: str, tag: str) -> int:\n",
    "    num_with_tag = re.findall(tag, text)\n",
    "    return len(num_with_tag)\n",
    "\n",
    "# Apply count NUMs with <NUM> tag\n",
    "num_tag = '_num_'\n",
    "clean_data['num_count'] = clean_data['content_clean'].apply(count_tag, tag=num_tag)\n",
    "\n",
    "# Apply count DATEs with <DATE> tag\n",
    "date_tag = '_date_'\n",
    "clean_data['date_count'] = clean_data['content_clean'].apply(count_tag, tag=date_tag)\n",
    "\n",
    "# Apply count URLs with <URL> tag\n",
    "url_tag = '_url_'\n",
    "clean_data['url_count'] = clean_data['content_clean'].apply(count_tag, tag=url_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count single char in string\n",
    "def count_char(text: str, char: str):\n",
    "    return text.count(',')\n",
    "\n",
    "# count of commas in each article\n",
    "comma = ','\n",
    "clean_data['comma_count'] = clean_data['content_clean'].apply(count_char, char=comma)\n",
    "\n",
    "# count of exlamation points in each article\n",
    "exclm = '!'\n",
    "clean_data['exclm_count'] = clean_data['content_clean'].apply(count_char, char=exclm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import swifter\n",
    "\n",
    "# Count unique words in text (word frequency of content_clean)\n",
    "def get_word_freq(text: str) -> int:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(set(tokens))\n",
    "\n",
    "# get word freq\n",
    "clean_data['content_word_freq'] = clean_data['content_clean'].swifter.apply(get_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import swifter\n",
    "\n",
    "# Count unique words in text (word frequency of content_clean)\n",
    "def get_word_freq(text: str) -> int:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return len(set(tokens))\n",
    "\n",
    "# word freq after stopword removal\n",
    "clean_data['stop_word_freq'] = clean_data['content_stopword'].swifter.apply(get_word_freq)\n",
    "\n",
    "# word freq after stemming\n",
    "clean_data['stem_word_freq'] = clean_data['content_stem'].swifter.apply(get_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction rate on stopword removal\n",
    "# training\n",
    "col_a = clean_data['content_word_freq']\n",
    "col_b = clean_data['stop_word_freq']\n",
    "clean_data['stop_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction rate on stem removal\n",
    "# training\n",
    "col_a = clean_data['content_word_freq']\n",
    "col_b = clean_data['stem_word_freq']\n",
    "clean_data['stem_reduction_rate'] = round(((col_a - col_b)/col_a) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Avarage of use of words per sentence. per article)\n",
    "\n",
    "import swifter\n",
    "\n",
    "def average_sentence_length(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Initialize variables to store total length and number of sentences\n",
    "    total_length = 0\n",
    "    num_sentences = 0\n",
    "    \n",
    "    # Iterate through each sentence to calculate total length and count the number of sentences\n",
    "    for sentence in sentences:\n",
    "        # Count the number of words in the sentence\n",
    "        words = sentence.split()\n",
    "        length = len(words)\n",
    "        \n",
    "        # Add the length of the current sentence to the total length\n",
    "        total_length += length\n",
    "        \n",
    "        # Increment the number of sentences\n",
    "        if length > 0:  # Exclude empty sentences\n",
    "            num_sentences += 1\n",
    "    \n",
    "    # Calculate the average length of sentences\n",
    "    if num_sentences > 0:\n",
    "        average_length = total_length / num_sentences\n",
    "    else:\n",
    "        average_length = 0\n",
    "    \n",
    "    return int(average_length)\n",
    "\n",
    "# Apply\n",
    "clean_data['average_sentence_length'] = clean_data['content'].swifter.apply(average_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove random rows to balance dataset\n",
    "random_n = 249-208\n",
    "select_type = clean_data[ clean_data['type'] == 'false']\n",
    "random_rows = select_type.sample(random_n, random_state=28)\n",
    "clean_data = clean_data.drop(random_rows.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "dst = 'data/liar_dataset/test_features.csv'\n",
    "clean_data.to_csv(dst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake-news-proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
